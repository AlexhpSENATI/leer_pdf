import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import joblib
import nltk
from nltk.corpus import stopwords

# Descargar stopwords solo si no estÃ¡n descargadas
nltk.download('stopwords', quiet=True)

# Usar stopwords en espaÃ±ol
spanish_stopwords = stopwords.words('spanish')

# ğŸ“¥ Cargar dataset
try:
    df = pd.read_csv("dataset.csv")
except FileNotFoundError:
    print("âŒ Error: El archivo 'dataset.csv' no fue encontrado.")
    exit()

# ğŸ§¹ Eliminar filas con campos vacÃ­os
df.dropna(subset=["texto", "area"], inplace=True)

# ğŸš¨ Verificar si el dataset tiene suficientes datos
if df.empty or df["area"].nunique() < 2:
    print("âš ï¸ El dataset es muy pequeÃ±o o no tiene suficientes clases. Agrega mÃ¡s datos.")
    exit()

# âš™ï¸ Crear pipeline de procesamiento y modelo
modelo = Pipeline([
    ("vectorizer", TfidfVectorizer(stop_words=spanish_stopwords)),
    ("clf", MultinomialNB())
])

# ğŸ“ Entrenar modelo
modelo.fit(df["texto"], df["area"])

# ğŸ’¾ Guardar modelo entrenado
joblib.dump(modelo, "modelo_entrenado.pkl")

print("âœ… Modelo entrenado y guardado correctamente.")


# import pandas as pd
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.pipeline import Pipeline
# import joblib
# import nltk
# from nltk.corpus import stopwords

# nltk.download('stopwords')

# spanish_stopwords = stopwords.words('spanish')

# # Cargar dataset
# df = pd.read_csv("dataset.csv")

# df = df.dropna(subset=["texto", "area"])

# modelo = Pipeline([
#     ("vectorizer", TfidfVectorizer(stop_words=spanish_stopwords)),
#     ("clf", MultinomialNB())
# ])

# modelo.fit(df["texto"], df["area"])

# # Guardar modelo entrenado en archivo
# joblib.dump(modelo, "modelo_entrenado.pkl")

# print("âœ… Modelo entrenado y guardado correctamente.")
